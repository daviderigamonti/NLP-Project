{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ce1016-35e5-49cf-8c91-170b46d7b7df",
   "metadata": {},
   "source": [
    "# Natural Language Processing Project\n",
    "## NLP Course @ Politecnico di Milano 2022/2023 - Prof. Mark Carman\n",
    "### Topic 4: Autextification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827eff8a-355b-4bcc-ab68-c039bc6aa153",
   "metadata": {},
   "source": [
    "### Group: Residual Sum of Students\n",
    "- Raul Singh\n",
    "- Davide Rigamonti\n",
    "- Francesco Tosini\n",
    "- Enrico Zuccolotto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41621b4b-6956-4014-9154-f1e0b03f451d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The dataset consists of *short text passages* that have either been written by a *human* or have been generated automatically by a *language model*, more information can be found on the [official site](https://sites.google.com/view/autextification).\n",
    "\n",
    "The original task will take place as part of [IberLEF 2023](http://sepln2023.sepln.org/en/iberlef-en/), the 5th Workshop on Iberian Languages Evaluation Forum at the SEPLN 2023 Conference, which will be held in Jaén, Spain on the 26th of September, 2023.\n",
    "\n",
    "Given the scope of the original challenge we can observe that the dataset contains two separate set of samples, one in **English** and the other in **Spanish**; our main focus will be on the **English** dataset.\n",
    "\n",
    "We will treat the two tasks **separately** as the two respective goals are different; although some similarities can be traced between the two, most of the considered approaches will be symmetrical and net homogeneous results.\n",
    "\n",
    "Each task will be presented with a brief **data exploration** section, then we will proceed to utilize models and approaches that we have seen in the course (with the introduction of some novelties) starting from the most basic techniques based on the **Bag of Words representation** to then transition towards approaches that utilize **Word Embeddings** to then reach the *state-of-the-art* **Transformer** models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1402c1-9723-4b39-bad1-2160c5570b37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preliminary initialization\n",
    "\n",
    "This section contains all the library imports, helper function initialization calls and global variable definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8540e8-6fa0-4e62-9830-240ddd80e116",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5965d0-a5b6-4d67-8500-dd24f8f5a1c8",
   "metadata": {},
   "source": [
    "#### Utilized libraries\n",
    "The following dependencies are needed to run the notebook:\n",
    "```\n",
    "pip install scikit-learn~=1.2.2\n",
    "pip install torch~=2.0.0\n",
    "pip install matplotlib~=3.7.1\n",
    "pip install plotly~=5.14.1\n",
    "pip install nltk~=3.8.1\n",
    "pip install spacy~=3.5.1\n",
    "pip install textstat~=0.7.3\n",
    "pip install numpy~=1.24.2\n",
    "pip install pandas~=2.0.0\n",
    "pip install python-terrier~=0.9.2\n",
    "pip install scipy~=1.9.3\n",
    "pip install gensim~=4.3.1\n",
    "pip install lexicalrichness~=0.5.0\n",
    "pip install sentence-transformers~=2.2.2\n",
    "pip install transformers~=4.28.1\n",
    "pip install datasets~=2.12.0\n",
    "pip install evaluate~=0.4.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b9f32d-8503-4529-b166-7026fe5f00fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Python standard library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67862620-d6cb-49a8-a43d-9c757dc155f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import abc\n",
    "import random\n",
    "import string\n",
    "\n",
    "import copy as cp\n",
    "import array as arr\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017ffba-d718-4b78-8878-c63baba7f9e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0919f457-f9ff-4d0f-a263-b3eb294d0a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6707ec75-d9b5-4944-b880-011889575e54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535df294-54e2-43e0-8459-0325d09ca0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import torch.utils.data as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55efb1d4-703b-43ab-afad-62748a60a6bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b2a8a-b96d-4601-900a-da2ce73b0e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a50a7-cfdf-4ddc-b2c5-d4becf39a265",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Various"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4379d06-db1d-4d31-8eee-7686828d9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import textstat\n",
    "\n",
    "import evaluate as hf_ev\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import scipy.sparse as sps\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "from spacy import displacy\n",
    "from nltk.corpus import stopwords\n",
    "from pandas.core.common import flatten\n",
    "from datasets import Dataset, DatasetDict\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from lexicalrichness import LexicalRichness\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from nlp_project.notebook_utils import compact_split, evaluate\n",
    "\n",
    "from nlp_project.notebook_utils import evaluate, split, save_scikit_model, load_scikit_model\n",
    "from nlp_project.nn_utils import init_gpu\n",
    "from nlp_project.nn_classifier import Data, Classifier\n",
    "from nlp_project.nn_extra import EarlyStopping, AdaptLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d486853e-d211-40dc-82cc-ba64f404b5e7",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0678743-04c3-4acc-b4fe-0cb6f9b6c62c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Pytorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47a7b1-c31f-4a5b-9b1a-998781e6e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_metrics = {\n",
    "    \"epoch\": {},\n",
    "    \"loss\": {\"order\": -1},\n",
    "    \"val_loss\": {\"order\": -1},\n",
    "    \"acc\": {\"order\": +1},\n",
    "    \"val_acc\": {\"order\": +1},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d625bf-0226-4a92-a225-1245fc677544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopNNLoop(BaseException):\n",
    "    pass\n",
    "\n",
    "def build_history_string(history_point):\n",
    "    epoch = history_point[\"epoch\"]\n",
    "    metrics_string = \" \".join(\n",
    "        [f\"{k}: {history_point[k]:.7f}\" for k in history_point if not k == \"epoch\"]\n",
    "    )\n",
    "    return f\"Epoch {epoch} -- \" + metrics_string\n",
    "\n",
    "\n",
    "def compare_equal_models(model_1, model_2):\n",
    "    models_differ = 0\n",
    "    for key_item_1, key_item_2 in zip(\n",
    "        model_1.state_dict().items(), model_2.state_dict().items()\n",
    "    ):\n",
    "        if torch.equal(key_item_1[1], key_item_2[1]):\n",
    "            pass\n",
    "        else:\n",
    "            models_differ += 1\n",
    "            if key_item_1[0] == key_item_2[0]:\n",
    "                print(\"Mismtach found at\", key_item_1[0])\n",
    "            else:\n",
    "                raise Exception\n",
    "    if models_differ == 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Returns true if a is \"better\" than b following the metric\n",
    "def compare_metric(metric, a, b, delta=0):\n",
    "    if a == b:\n",
    "        return False\n",
    "    if history_metrics[metric][\"order\"] == +1:\n",
    "        return a > b + delta\n",
    "    return a < b - delta\n",
    "\n",
    "\n",
    "# Initializes lowest possible value given a metric\n",
    "def init_lowest(metric):\n",
    "    return -np.inf if history_metrics[metric][\"order\"] == +1 else np.inf\n",
    "\n",
    "\n",
    "def init_gpu(gpu=\"cuda:0\"):\n",
    "    return torch.device(gpu if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b3d28-573b-49e4-849a-85d76b1e30d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(dt.Dataset):\n",
    "    def __init__(self, x, y, x_type=np.int32, y_type=torch.float):\n",
    "        x_coo = x.tocoo()\n",
    "        self.x = torch.sparse.FloatTensor(\n",
    "            torch.LongTensor([x_coo.row, x_coo.col]),\n",
    "            torch.FloatTensor(x_coo.data.astype(x_type)),\n",
    "            x_coo.shape,\n",
    "        )\n",
    "        self.y = torch.tensor(y, dtype=y_type)\n",
    "        self.shape = self.x.shape\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index].to_dense(), self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162e0b5-7b9d-444e-a5c4-1a584bdd1cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, binary_classifier=False, device=torch.device(\"cpu\"), verbose=True):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.is_binary = binary_classifier\n",
    "        self.verbose = verbose\n",
    "        self.is_compiled = False\n",
    "        self.history = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "    def compile(self, loss, optimizer, binary_threshold=0.5):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.binary_threshold = binary_threshold\n",
    "        self.to(self.device)\n",
    "        self.is_compiled = True\n",
    "\n",
    "    def parse_logits(self, outputs):\n",
    "        if self.is_binary:\n",
    "            predicted = (outputs > self.binary_threshold).float()\n",
    "        else:\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        return predicted\n",
    "\n",
    "    def train_loop(self, data, epochs, data_val=None, callbacks=[]):\n",
    "        try:\n",
    "            tot = len(data.dataset)\n",
    "            # Iterate over all epochs\n",
    "            for epoch in range(epochs):\n",
    "                running_loss = 0.0\n",
    "                correct = 0\n",
    "                history_point = {}\n",
    "                # Iterate over each dataset batch\n",
    "                for i, datum in enumerate(data):\n",
    "                    # Decompose batch in x and y\n",
    "                    inputs, labels = datum\n",
    "                    # Set gradients to zero\n",
    "                    self.optimizer.zero_grad()\n",
    "                    # Forward pass\n",
    "                    outputs = self(inputs)\n",
    "                    predictions = self.parse_logits(outputs)\n",
    "                    current_loss = self.loss(outputs, labels)\n",
    "                    # Backpropagation\n",
    "                    current_loss.backward()\n",
    "                    # Optimization\n",
    "                    self.optimizer.step()\n",
    "                    # Update metrics\n",
    "                    running_loss += current_loss.item()\n",
    "                    correct += (predictions == labels).float().sum()\n",
    "\n",
    "                # Compute training metrics\n",
    "                history_point[\"epoch\"] = epoch + 1\n",
    "                history_point[\"loss\"] = running_loss / tot\n",
    "                history_point[\"acc\"] = correct / tot\n",
    "\n",
    "                # Compute and save eventual validation metrics\n",
    "                if data_val:\n",
    "                    _, val_metrics = self.test_loop(data_val)\n",
    "                    history_point[\"val_loss\"] = val_metrics[\"loss\"]\n",
    "                    history_point[\"val_acc\"] = val_metrics[\"acc\"]\n",
    "\n",
    "                # Save epoch in history\n",
    "                self.history.append(history_point)\n",
    "\n",
    "                # Perform callbacks\n",
    "                for callback in callbacks:\n",
    "                    callback.call(self, history_point)\n",
    "\n",
    "                # Print epoch summary\n",
    "                if self.verbose:\n",
    "                    print(build_history_string(history_point))\n",
    "\n",
    "        except StopNNLoop as s:  # noqa\n",
    "            pass\n",
    "\n",
    "    def test_loop(self, data):\n",
    "        all_predictions = np.array([])\n",
    "        tot = len(data.dataset)\n",
    "        loss = 0.0\n",
    "        correct = 0\n",
    "        metrics = {}\n",
    "        # Prevent model update\n",
    "        with torch.no_grad():\n",
    "            # Iterate over each dataset batch\n",
    "            for datum in data:\n",
    "                # Decompose batch in x and y\n",
    "                inputs, labels = datum\n",
    "                # Forward pass\n",
    "                outputs = self(inputs)\n",
    "                predictions = self.parse_logits(outputs)\n",
    "                current_loss = self.loss(outputs, labels)\n",
    "                # Update metrics\n",
    "                loss += current_loss.item()\n",
    "                correct += (predictions == labels).float().sum()\n",
    "                # Append predictions\n",
    "                all_predictions = np.append(all_predictions, predictions)\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics[\"acc\"] = correct / tot\n",
    "        metrics[\"loss\"] = loss / tot\n",
    "\n",
    "        return all_predictions.flatten(), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91e6161-bbe1-452b-8777-151b7eb45ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, inputs):\n",
    "        if not isinstance(inputs, list):\n",
    "            raise TypeError(\"Parameter 'inputs' must be a list\")\n",
    "        if not all(x in history_metrics for x in inputs):\n",
    "            raise ValueError(\n",
    "                \"Unknown input value, not present in Callback.callback_inputs\"\n",
    "            )\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def inputs_check(self, inputs):\n",
    "        if not all(x in inputs for x in self.inputs):\n",
    "            raise ValueError(\n",
    "                f\"Requested inputs not provided: {[i for i in inputs if i not in self.inputs]}\"\n",
    "            )\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def call(self, model, inputs):\n",
    "        self.inputs_check(inputs)\n",
    "        pass\n",
    "\n",
    "class EarlyStopping(Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        metric=\"loss\",\n",
    "        patience=10,\n",
    "        baseline=None,\n",
    "        delta=0,\n",
    "        restore_best=True,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        super().__init__([metric])\n",
    "        self.metric = metric\n",
    "        self.patience = patience\n",
    "        self.baseline = baseline\n",
    "        self.delta = delta\n",
    "        self.restore_best = restore_best\n",
    "        self.verbose = verbose\n",
    "        self.best_epoch = 0\n",
    "        self.counter = 0\n",
    "        self.saved_params = {}\n",
    "        self.last_best = init_lowest(self.metric)\n",
    "\n",
    "    def call(self, model, inputs):\n",
    "        super().call(model, inputs)\n",
    "        if self.early_stop(model, inputs):\n",
    "            raise StopNNLoop()\n",
    "\n",
    "    def early_stop(self, model, inputs):\n",
    "        metric = inputs[self.metric]\n",
    "        # Check if new metric is better than the current best\n",
    "        if compare_metric(self.metric, metric, self.last_best):\n",
    "            # Reset counter and update best value\n",
    "            self.last_best = metric\n",
    "            self.counter = 0\n",
    "            self.best_epoch = inputs[\"epoch\"]\n",
    "            # Update model checkpoint\n",
    "            if compare_metric(self.metric, metric, self.baseline):\n",
    "                self.saved_params = cp.deepcopy(model.state_dict())\n",
    "        # Check if new metric is worse than the current best\n",
    "        elif compare_metric(self.metric, self.last_best, metric, self.delta):\n",
    "            # Increment counter\n",
    "            self.counter += 1\n",
    "            # Check if counter exceeds patience, if so interrupt training\n",
    "            if self.counter >= self.patience:\n",
    "                # Restore best model checkpoint if possible and wanted\n",
    "                if not self.restore_best:\n",
    "                    return True\n",
    "                if self.saved_params:\n",
    "                    model.load_state_dict(self.saved_params)\n",
    "                if self.verbose:\n",
    "                    if self.saved_params:\n",
    "                        print(f\"Model restored successfully @ epoch {self.best_epoch}\")\n",
    "                    else:\n",
    "                        print(f\"Couldn't restore model @ epoch {self.best_epoch}\")\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "class AdaptLR(Callback):\n",
    "    def __init__(self, metric=\"loss\", patience=5, factor=0.1, delta=0, verbose=True):\n",
    "        super().__init__([metric])\n",
    "        self.metric = metric\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.last_best = init_lowest(self.metric)\n",
    "\n",
    "    def call(self, model, inputs):\n",
    "        super().call(model, inputs)\n",
    "        if self.adaptlr(inputs):\n",
    "            # Adapt learning rate\n",
    "            out = []\n",
    "            for g in model.optimizer.param_groups:\n",
    "                g[\"lr\"] *= self.factor\n",
    "                out = g[\"lr\"]\n",
    "            if self.verbose:\n",
    "                print(f\"Reducing lr to {out:.4f}\")\n",
    "\n",
    "    def adaptlr(self, inputs):\n",
    "        metric = inputs[self.metric]\n",
    "        # Check if new metric is better than the current best\n",
    "        if compare_metric(self.metric, metric, self.last_best):\n",
    "            # Reset counter and update best value\n",
    "            self.last_best = metric\n",
    "            self.counter = 0\n",
    "        # Check if new metric is worse than the current best\n",
    "        elif compare_metric(self.metric, self.last_best, metric, self.delta):\n",
    "            # Increment counter\n",
    "            self.counter += 1\n",
    "            # Check if counter exceeds patience, if so interrupt training\n",
    "            if self.counter >= self.patience:\n",
    "                self.counter = 0\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace11272-c716-42c3-8d42-6e242fde071f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Generic utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077b9ea-3354-4fa4-b7e0-caeba1d0d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x, y, test_size=0.2, val_size=0.0, seed=0):\n",
    "    if val_size + test_size >= 1:\n",
    "        return None\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=test_size + val_size, stratify=y, random_state=seed\n",
    "    )\n",
    "    x_val, y_val = None, None\n",
    "    if val_size > 0:\n",
    "        x_test, x_val, y_test, y_val = train_test_split(\n",
    "            x_test,\n",
    "            y_test,\n",
    "            test_size=val_size / (test_size + val_size),\n",
    "            stratify=y_test,\n",
    "            random_state=seed,\n",
    "        )\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n",
    "def compact_split(dataset, test_size=0.2, val_size=0.0, seed=0):\n",
    "    if val_size + test_size >= 1:\n",
    "        return None\n",
    "    train, test = train_test_split(\n",
    "        dataset, test_size=test_size + val_size, random_state=seed\n",
    "    )\n",
    "    val = None\n",
    "    if val_size > 0:\n",
    "        val, test = train_test_split(\n",
    "            test,\n",
    "            test_size=test_size / (test_size + val_size),\n",
    "            random_state=seed,\n",
    "        )\n",
    "    return train, val, test\n",
    "\n",
    "def evaluate(y_true, y_pred, labels=None):\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    cm_display.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cecd9a-2f1d-43f8-b514-353500866aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cv_models(models, x_train, y_train):\n",
    "    for model in models:\n",
    "        x_train_, y_train_ = x_train, y_train\n",
    "        if \"subsample\" in model.keys():\n",
    "            x_train_, _, y_train_, _ = train_test_split(\n",
    "                x_train, \n",
    "                y_train, \n",
    "                test_size=model[\"subsample\"], \n",
    "                stratify=y_train\n",
    "            )\n",
    "                \n",
    "        print(f\"Training {model['name']}\")        \n",
    "        model[\"model\"].fit(x_train_, y_train_)\n",
    "        \n",
    "        print(\"Found best model\")\n",
    "        model[\"best\"] = model[\"model\"].best_estimator_\n",
    "        model[\"best\"].fit(x_train, y_train)\n",
    "        print(\"Trained best model\")\n",
    "\n",
    "def test_cv_models(models, x_test, y_test):\n",
    "    for model in models:\n",
    "    print(f\"{model['name']}\")\n",
    "    if hasattr(model[\"model\"], \"cv_results_\"):\n",
    "        print(f\"Best parameters: {model['model'].best_params_}\")\n",
    "        print(f\"Best CV score: {model['model'].best_score_}\")\n",
    "    y_pred = model['best'].predict(x_test)\n",
    "    evaluate(y_test, y_pred, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675eea3-5d1b-44c1-a83e-9cdcdfa8c92b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Variable definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375ffce-e431-49fd-909d-916b60cdefce",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c24b6-3c8f-409a-ac32-385bf28313f9",
   "metadata": {},
   "source": [
    "#### Library initialization calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a81524a-b7b3-4d23-8da6-f2b1236f6690",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Load spacy pipeline model\n",
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00998b82-302e-4c79-9c0b-c911f30753cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 1\n",
    "\n",
    "The challange is subdivided in two main tasks, the first is a **Binary Classification** task that aims at identifying if a text passage was *written by a human* or if it was *generated from a langauge model*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94aac4e-03f4-4370-ada5-7427712fd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification labels for Task 1\n",
    "labels = [\"generated\", \"human\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ef8164-1de1-4afd-98cd-f050a1335924",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf7467b-35e1-4085-9c10-4c04499ebe0a",
   "metadata": {},
   "source": [
    "We load the english dataset and we run the *en_core_web_sm* SpaCy pipeline on it to generate a vectorized representation enriched with POS tags and NER tags.\n",
    "\n",
    "The [SpaCy pipeline](https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-3.5.0) contains the following components: *tok2vec*, *tagger*, *parser*, *senter*, *attribute_ruler*, *lemmatizer*, *ner*.\n",
    "\n",
    "In addition, we define subsets of grouped POS and NER tags that may be of interest to our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf0fd22-f7c4-4cc7-94a1-56e288c9454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_csv(\"../AUTEXTIFICATION/subtask_1/train.tsv\", sep=\"\\t\")\n",
    "df = df.drop(\"id\", axis=1)\n",
    "df[\"tagged_text\"] = df[\"text\"].apply(lambda x: nlp_model(x))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44f698-5c2c-4fd1-91ef-6f5a959b7efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting POS tags\n",
    "sel_pos = {\n",
    "    \",\": [\",\"], \".\": [\",\"], \":\": [\":\"], \"ADD\": [\"ADD\"], \"AFX\": [\"AFX\"], \"CC\": [\"CC\"],\n",
    "    \"CD\": [\"CD\"], \"DT\": [\"DT\"], \"EX\": [\"EX\"], \"HYPH\": [\"HYPH\"], \"IN\": [\"IN\"],\n",
    "    \"JJ\": [\"JJ\", \"JJR\", \"JJS\"], \"NFP\": [\"NFP\"], \"NN\": [\"NN\", \"NNP\", \"NNPS\", \"NNS\"],\n",
    "    \"PRP\": [\"PRP\", \"PRP$\"], \"RB\": [\"RB\", \"RBR\", \"RBS\", \"RP\"], \"SYM\": [\"SYM\"], \"TO\": [\"TP\"],\n",
    "    \"UH\": [\"UH\"], \"VB\": [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"], \"WDT\": [\"WDT\"], \"XX\": [\"XX\"]\n",
    "}\n",
    "\n",
    "# Interesting NER tags\n",
    "sel_ner = [\n",
    "    \"CARDINAL\", \"DATE\", \"EVENT\", \"FAC\", \"GPE\", \"LANGUAGE\", \"LAW\", \"LOC\", \"MONEY\", \"NORP\",\n",
    "    \"ORDINAL\", \"ORG\", \"PERCENT\", \"PERSON\", \"PRODUCT\", \"QUANTITY\", \"TIME\", \"WORK_OF_ART\"\n",
    "]\n",
    "\n",
    "# Lower limit for the number of samples to consider\n",
    "limit_n_samples = 40\n",
    "\n",
    "# Print tag explanation\n",
    "print(\"POS tags\")\n",
    "for tag in sel_pos:\n",
    "    print(f\"{tag} {sel_pos[tag]}: {spacy.explain(tag)}\")\n",
    "print(\"NER tags\")\n",
    "for tag in sel_ner:\n",
    "    print(f\"{tag}: {spacy.explain(tag)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f9a76-5ded-43b5-b4c5-da52714b0e44",
   "metadata": {},
   "source": [
    "We enrich the dataset calculating some custom metrics for each sentence, such as:\n",
    "- text length\n",
    "- number of uppercase letters\n",
    "- number of different and cumulative stopwords\n",
    "- POS and NER tag counts\n",
    "  \n",
    "Most of these metrics have been calculated considering both an absolute and a relative (to the text length) approach.\n",
    "\n",
    "Then, we delete metrics that don't meet a given support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da47e1e3-9d3e-4703-b0b8-f43b35828866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length\n",
    "df[\"length\"] = df[\"text\"].str.len()\n",
    "\n",
    "# Number of uppercase letters\n",
    "df[\"n_upcase\"] = df[\"text\"].str.count(r\"[A-Z]\")\n",
    "df[\"n_upcase_rel\"] = df[\"n_upcase\"] / df[\"length\"]\n",
    "\n",
    "# Number of stopwords\n",
    "df[\"n_stopword\"] = df[\"text\"].str.split().apply(\n",
    "    lambda x: len(set(x) & set(stopwords.words(\"english\")))\n",
    ")\n",
    "df[\"ncum_stopword\"] = df[\"text\"].str.split().apply(\n",
    "    lambda x: len([w for w in x if w in stopwords.words(\"english\")])\n",
    ")\n",
    "df[\"n_stopword_rel\"] = df[\"n_stopword\"] / df[\"length\"]\n",
    "df[\"ncum_stopword_rel\"] = df[\"ncum_stopword\"] / df[\"length\"]\n",
    "\n",
    "# Number of POS and NER tags\n",
    "for tag in sel_pos:\n",
    "    column_tag = \"n_pos_\" + tag\n",
    "    df[column_tag] = df[\"tagged_text\"].apply(\n",
    "        lambda x: len([tok for tok in x if tok.tag_ in sel_pos[tag]])\n",
    "    )\n",
    "    df[column_tag + \"_rel\"] = df[column_tag] / df[\"length\"]\n",
    "for tag in sel_ner:\n",
    "    column_tag = \"n_ner_\" + tag\n",
    "    df[column_tag] = df[\"tagged_text\"].apply(\n",
    "        lambda x: len([tok for tok in x if tok.ent_type_ == tag])\n",
    "    )\n",
    "    df[column_tag + \"_rel\"] = df[column_tag] / df[\"length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993bd67e-fcb2-48b6-8ba1-6b9c4ae709be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deleting empty columns\")\n",
    "print(df.columns[(df == 0).all(axis=0)].tolist())\n",
    "df = df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "print(f\"Deleting columns with less than {limit_n_samples} samples\")\n",
    "print(df.columns[df.astype(bool).sum(axis=0) <= limit_n_samples].tolist())\n",
    "df = df.loc[:, df.astype(bool).sum(axis=0) > limit_n_samples]\n",
    "\n",
    "# Defragment dataset\n",
    "all_columns = df.columns.tolist()\n",
    "df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20679366-3bcf-40bd-9779-610f67aa45e7",
   "metadata": {},
   "source": [
    "We define a function to neatly plot comparisons between human and generated samples utilizing the metrics visualized as box plots and histograms.\n",
    "\n",
    "We opt to ignore metrics that result too similar between the two classes comparing the 1/4, 1/2 and 3/4 quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7351c803-10e0-40ff-bb5e-c0e56d6a8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_df_stats(\n",
    "    df,\n",
    "    sel_columns,\n",
    "    labels,\n",
    "    colors,\n",
    "    ignore_similar=True,\n",
    "    height=1000,\n",
    "    width=800\n",
    "):\n",
    "    ignored = []\n",
    "    vis_columns = []\n",
    "    for column in sel_columns:\n",
    "        if ignore_similar:\n",
    "            col_name = column[0][\"name\"]\n",
    "            x = df[col_name]\n",
    "            x1 = df.loc[df['label']==labels[0]][col_name]\n",
    "            x2 = df.loc[df['label']==labels[1]][col_name]\n",
    "            delta = (x.max() - x.min()) / 100\n",
    "            if (np.abs(x1.quantile(0.25) - x2.quantile(0.25)) <= delta and\n",
    "                np.abs(x1.quantile(0.5) - x2.quantile(0.5)) <= delta and \n",
    "                np.abs(x1.quantile(0.75) - x2.quantile(0.75)) <= delta):\n",
    "                ignored.append(col_name)\n",
    "                continue\n",
    "        vis_columns.append(column)\n",
    "        \n",
    "    titles = [c[\"name\"] for c_arr in vis_columns for c in c_arr]\n",
    "    specs = [\n",
    "        [{\"secondary_y\": True} if h[\"type\"] == \"hist\" else {} for h in s] \n",
    "        if len(s) > 1 else [{\"colspan\":2}, None]\n",
    "        for s in vis_columns\n",
    "    ]\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        horizontal_spacing = 0.005,\n",
    "        vertical_spacing = 0.01,\n",
    "        rows=len(vis_columns),\n",
    "        cols=2,\n",
    "        subplot_titles=titles,\n",
    "        specs=specs\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        height=height,\n",
    "        width=width,\n",
    "        showlegend=False,\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "\n",
    "    for i, column in enumerate(vis_columns):\n",
    "        if len(column) > 1:\n",
    "            for j, subcolumn in enumerate(column):\n",
    "                x1 = df.loc[df['label']==labels[0]][subcolumn[\"name\"]]\n",
    "                x2 = df.loc[df['label']==labels[1]][subcolumn[\"name\"]]\n",
    "                if subcolumn[\"type\"] == \"box\":\n",
    "                    add_boxplot(fig, [x1, x2], i+1, j+1, labels, colors)\n",
    "                elif subcolumn[\"type\"] == \"hist\":\n",
    "                    add_hist(fig, [x1, x2], i+1, j+1, labels, colors)\n",
    "                else:\n",
    "                    raise Exception()\n",
    "        else:\n",
    "            column = column[0]\n",
    "            x1 = df.loc[df['label']==labels[0]][column[\"name\"]]\n",
    "            x2 = df.loc[df['label']==labels[1]][column[\"name\"]]\n",
    "            add_boxplot(fig, [x1, x2], i+1, 1, labels, colors)\n",
    "\n",
    "    fig.show()\n",
    "    print(f\"{[i for i in ignored]} too similar, ignored\")\n",
    "    \n",
    "def add_boxplot(fig, x, row, col, labels, colors):\n",
    "    for i, el in enumerate(x):\n",
    "        fig.add_trace(go.Box(\n",
    "            y=el,\n",
    "            name=labels[i],\n",
    "            marker_color=colors[i]\n",
    "        ),row=row, col=col)\n",
    "        \n",
    "def add_hist(fig, x, row, col, labels, colors):\n",
    "    offset = len(x)\n",
    "    temp = ff.create_distplot(x, labels, curve_type = 'kde')\n",
    "    normal_x = []\n",
    "    normal_y = []\n",
    "    for n in range(offset):\n",
    "        normal_x.append(temp.data[offset + n]['x'])\n",
    "        normal_y.append(temp.data[offset + n]['y'])\n",
    "    for i, el in enumerate(x):\n",
    "        fig.add_trace(go.Histogram(\n",
    "            x=el,\n",
    "            orientation=\"v\",\n",
    "            xbins=go.histogram.XBins(size=(max(el) - min(el)) / 15),\n",
    "            name=labels[i],\n",
    "            opacity=0.4,\n",
    "            marker_color=colors[i]\n",
    "        ), row=row, col=col)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=normal_x[i],\n",
    "            y=normal_y[i],\n",
    "            mode = 'lines',\n",
    "            name=labels[i],\n",
    "            marker_color=colors[i]\n",
    "        ), row=row, col=col, secondary_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf92219-3c31-478e-99cd-1a72d17ac1f2",
   "metadata": {},
   "source": [
    "We select the metrics that we want to visualize and how we want to visualize them.\n",
    "\n",
    "For the sake of brevity we only choose to visualize some of the POS/NER tags here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e1117-6548-4b7f-929d-1b0b1d6e8894",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_columns = [\n",
    "    [{\"name\": \"length\", \"type\": \"box\"}], \n",
    "    [{\"name\": \"n_upcase\", \"type\": \"box\"}, {\"name\": \"n_upcase_rel\", \"type\": \"hist\"}], \n",
    "    [{\"name\": \"n_stopword\", \"type\": \"box\"}, {\"name\": \"n_stopword_rel\", \"type\": \"hist\"}],\n",
    "    [{\"name\": \"ncum_stopword\", \"type\": \"box\"}, {\"name\": \"ncum_stopword_rel\", \"type\": \"hist\"}],\n",
    "]\n",
    "\n",
    "# sel_columns.extend([\n",
    "#     [{\"name\": \"n_pos_\" + x, \"type\": \"box\"}, {\"name\": \"n_pos_\" + x + \"_rel\", \"type\": \"hist\"}]\n",
    "#     for x in sel_pos if \"n_pos_\" + x in all_columns\n",
    "# ])\n",
    "sel_columns.extend([\n",
    "    [{\"name\": \"n_pos_,\", \"type\": \"box\"}, {\"name\": \"n_pos_,_rel\", \"type\": \"hist\"}],\n",
    "    [{\"name\": \"n_pos_.\", \"type\": \"box\"}, {\"name\": \"n_pos_._rel\", \"type\": \"hist\"}],\n",
    "    [{\"name\": \"n_pos_CC\", \"type\": \"box\"}, {\"name\": \"n_pos_CC_rel\", \"type\": \"hist\"}],\n",
    "    [{\"name\": \"n_pos_CD\", \"type\": \"box\"}, {\"name\": \"n_pos_CD_rel\", \"type\": \"hist\"}],\n",
    "    [{\"name\": \"n_pos_DT\", \"type\": \"box\"}, {\"name\": \"n_pos_DT_rel\", \"type\": \"hist\"}],\n",
    "    [{\"name\": \"n_pos_JJ\", \"type\": \"box\"}, {\"name\": \"n_pos_JJ_rel\", \"type\": \"hist\"}],\n",
    "    [{\"name\": \"n_pos_NN\", \"type\": \"box\"}, {\"name\": \"n_pos_NN_rel\", \"type\": \"hist\"}],\n",
    "    [{\"name\": \"n_pos_VB\", \"type\": \"box\"}, {\"name\": \"n_pos_VB_rel\", \"type\": \"hist\"}],\n",
    "    [{\"name\": \"n_pos_WDT\", \"type\": \"box\"}, {\"name\": \"n_pos_WDT_rel\", \"type\": \"hist\"}],\n",
    "])\n",
    "\n",
    "# sel_columns.extend(\n",
    "#     [{\"name\": \"n_ner_\" + x, \"type\": \"box\"}, {\"name\": \"n_ner_\" + x + \"_rel\", \"type\": \"hist\"}]\n",
    "#     for x in sel_ner if \"n_ner_\" + x in all_columns\n",
    "# )\n",
    "sel_columns.extend([\n",
    "    [{\"name\": \"n_ner_DATE\", \"type\": \"box\"}, {\"name\": \"n_ner_DATE_rel\", \"type\": \"hist\"}],\n",
    "    [{\"name\": \"n_ner_GPE\", \"type\": \"box\"}, {\"name\": \"n_ner_GPE_rel\", \"type\": \"hist\"}],\n",
    "    [{\"name\": \"n_ner_LAW\", \"type\": \"box\"}, {\"name\": \"n_ner_LAW_rel\", \"type\": \"hist\"}],\n",
    "])\n",
    "\n",
    "plot_df_stats(\n",
    "    df,\n",
    "    sel_columns,\n",
    "    labels=labels,\n",
    "    colors=[\"darkorchid\", \"forestgreen\"],\n",
    "    height=8000, width=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b84e4aa-2b1d-4e13-8508-ba03a1513f3f",
   "metadata": {},
   "source": [
    "We repeat the process, this time adding interesting [Textstat](https://pypi.org/project/textstat/) and [LexicalRichness](https://lexicalrichness.readthedocs.io/) metrics.\n",
    "\n",
    "It's possible to observe how most of the visualized metrics, are actually quite similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2d0ee0-0d42-4b4d-b0a2-96fe4c551011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textstat metrics https://pypi.org/project/textstat/\n",
    "textstat_metrics = [\n",
    "    'flesch_reading_ease', 'flesch_kincaid_grade', 'smog_index', 'coleman_liau_index',\n",
    "    'automated_readability_index', 'dale_chall_readability_score', 'difficult_words',\n",
    "    'linsear_write_formula', 'gunning_fog', 'fernandez_huerta', 'szigriszt_pazos', \n",
    "    'gutierrez_polini', 'crawford', 'gulpease_index', 'osman'\n",
    "]\n",
    "df['flesch_reading_ease'] = df[\"text\"].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "df['flesch_kincaid_grade'] = df[\"text\"].apply(lambda x: textstat.flesch_kincaid_grade(x))\n",
    "df['smog_index'] = df[\"text\"].apply(lambda x: textstat.smog_index(x))\n",
    "df['coleman_liau_index'] = df[\"text\"].apply(lambda x: textstat.coleman_liau_index(x))\n",
    "df['automated_readability_index'] = df[\"text\"].apply(lambda x: textstat.automated_readability_index(x))\n",
    "df['dale_chall_readability_score'] = df[\"text\"].apply(lambda x: textstat.dale_chall_readability_score(x))\n",
    "df['difficult_words'] = df[\"text\"].apply(lambda x: textstat.difficult_words(x))\n",
    "df['linsear_write_formula'] = df[\"text\"].apply(lambda x: textstat.linsear_write_formula(x))\n",
    "df['gunning_fog'] = df[\"text\"].apply(lambda x: textstat.gunning_fog(x))\n",
    "df['fernandez_huerta'] = df[\"text\"].apply(lambda x: textstat.fernandez_huerta(x))\n",
    "df['szigriszt_pazos'] = df[\"text\"].apply(lambda x: textstat.szigriszt_pazos(x))\n",
    "df['gutierrez_polini'] = df[\"text\"].apply(lambda x: textstat.gutierrez_polini(x))\n",
    "df['crawford'] = df[\"text\"].apply(lambda x: textstat.crawford(x))\n",
    "df['gulpease_index'] = df[\"text\"].apply(lambda x: textstat.gulpease_index(x))\n",
    "df['osman'] = df[\"text\"].apply(lambda x: textstat.osman(x))\n",
    "\n",
    "# Lexicalrichness metrics https://lexicalrichness.readthedocs.io/\n",
    "lexrich_metrics = [\n",
    "    'ttr', 'rttr', 'cttr'\n",
    "]\n",
    "df['ttr'] = df[\"text\"].apply(lambda x: LexicalRichness(x).ttr)\n",
    "df['rttr'] = df[\"text\"].apply(lambda x: LexicalRichness(x).rttr)\n",
    "df['cttr'] = df[\"text\"].apply(lambda x: LexicalRichness(x).cttr)\n",
    "\n",
    "print(\"Deleting empty columns\")\n",
    "print(df.columns[(df == 0).all(axis=0)].tolist())\n",
    "df = df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "print(f\"Deleting columns with less than {limit_n_samples} samples\")\n",
    "print(df.columns[df.astype(bool).sum(axis=0) <= limit_n_samples].tolist())\n",
    "df = df.loc[:, df.astype(bool).sum(axis=0) > limit_n_samples]\n",
    "\n",
    "# Defragment dataset\n",
    "all_columns = df.columns.tolist()\n",
    "df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d61d95b-86ba-4546-a370-84a86342b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_columns = []\n",
    "\n",
    "sel_columns.extend([\n",
    "    [{\"name\": x, \"type\": \"box\"}] for x in textstat_metrics if x in all_columns\n",
    "])\n",
    "sel_columns.extend([\n",
    "    [{\"name\": x, \"type\": \"box\"}] for x in lexrich_metrics if x in all_columns\n",
    "])\n",
    "\n",
    "plot_df_stats(\n",
    "    df,\n",
    "    sel_columns,\n",
    "    labels=labels,\n",
    "    colors=[\"darkorchid\", \"forestgreen\"],\n",
    "    height=6000, width=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311aefcc-aa9d-4bf4-b1ac-4e2ff099d6d8",
   "metadata": {},
   "source": [
    "### Bag of Words-based models and other approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44001af1-4798-4267-8802-20f3d8aee4f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Only Bag of Words text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9792b9-c8be-4746-b4fb-dd34e52a27d5",
   "metadata": {},
   "source": [
    "We load the english dataset for the first task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8f243-e2d8-4bfd-9cbc-5689229a8c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import main dataset\n",
    "df = pd.read_csv(\"../AUTEXTIFICATION/subtask_1/train.tsv\", sep=\"\\t\")\n",
    "df = df.drop(\"id\", axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c885e070-690b-4963-8860-ad5b2c4fe08c",
   "metadata": {},
   "source": [
    "We define a preprocessing function that:\n",
    "- converts all of the data to lowercase\n",
    "- applies a regex to remove punctuation\n",
    "- vectorizes all of the words using a given vectorizer\n",
    "\n",
    "and we preprocess our data, splitting it into train and test sets utilizing the relevant helper function defined in [Preliminary initialization](#Preliminary-initialization).\n",
    "\n",
    "For most of the simple BoW approaches, we have found that utlizing a **TfidfVectorizer** leads to no particular improvement w.r.t. a **CountVectorizer**; in addition, setting a minimum term frequency of 4 is a good compromise between number of parameters and performance.\n",
    "\n",
    "Interestingly enough, keeping stopwords instead of removing them leads to slightly better results; this could be due to the fact that the difference in their usage is statistically relevant for the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd1806-4d18-4f14-8695-ee099d1b6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, lower=True, vectorizer=None, fit=True):\n",
    "    # Convert all text to lowercase\n",
    "    if lower:\n",
    "        data = [x.lower() for x in data]\n",
    "\n",
    "    # Remove punctuation and reset multiple spaces to one\n",
    "    punct_regex = re.compile(\"[\" + string.punctuation + \"\\’'\" + \"]\")\n",
    "    whitespace_regex = re.compile(\" ( )+\")\n",
    "    data = [whitespace_regex.sub(\" \", punct_regex.sub(\" \", x)) for x in data]\n",
    "    \n",
    "    # Vectorize\n",
    "    if vectorizer:\n",
    "        if fit:\n",
    "            data = vectorizer.fit_transform(data)\n",
    "        else:\n",
    "            data = vectorizer.transform(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab6763b-0f34-4651-83f3-2e064b33da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.6, ngram_range=(2,2))\n",
    "\n",
    "x, y = df[\"text\"], df[\"label\"]\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = split(\n",
    "    x, y, test_size=0.2, val_size=0.0, seed=seed\n",
    ")\n",
    "\n",
    "x_train = preprocess(x_train, vectorizer=vectorizer)\n",
    "x_test = preprocess(x_test, vectorizer=vectorizer, fit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e5a822-bfec-4973-99e8-2a79a51c875e",
   "metadata": {},
   "source": [
    "We define some simple models to try out the simple Bag of Words approach, without any additional data; the models that we are going to use are:\n",
    "- Multinomial Naive Bayes\n",
    "- Logistic Regression\n",
    "- C-Support SVM\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Extra Tree classifier\n",
    "\n",
    "All of the previous models are run using 5-fold cross-validation on a small gridsearch around some of their default parameters.\n",
    "\n",
    "To perform training and evaluation we have used the relevant helper functions defined in [Preliminary initialization](#Preliminary-initialization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf05451-1afe-4770-b937-88c777f1eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "usecached = False\n",
    "\n",
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb_param = {\"alpha\":[0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "nb_clf = GridSearchCV(nb, nb_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Naive Bayes\",\n",
    "    \"model\": nb_clf,\n",
    "})\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr_param = [{\n",
    "    \"solver\": [\"liblinear\"], \n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"C\":[0.01, 0.1, 1, 10]\n",
    "},{\n",
    "    \"solver\": (\"lbfgs\", \"sag\", \"saga\"), \n",
    "    \"penalty\": [\"l2\"],\n",
    "    \"C\":[0.01, 0.1, 1]\n",
    "}]\n",
    "lr_clf = GridSearchCV(lr, lr_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Linear Regression\",\n",
    "    \"model\": lr_clf,\n",
    "    \"subsample\": 0.7,\n",
    "})\n",
    "\n",
    "# SVC\n",
    "svc = SVC()\n",
    "svc_param = {\"kernel\": [\"rbf\"], \"C\": [0.1, 1, 10]}\n",
    "svc_clf = GridSearchCV(svc, svc_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"SVC\",\n",
    "    \"model\": svc_clf,\n",
    "    \"subsample\": 0.7,\n",
    "})\n",
    "\n",
    "# Decision Tree\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree_param = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"], \n",
    "    # \"min_samples_split\": [2, 4, 8],\n",
    "    # \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [None, \"sqrt\", \"log2\"],\n",
    "}\n",
    "dtree_clf =  GridSearchCV(dtree, dtree_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Decision Tree\",\n",
    "    \"model\": dtree_clf,\n",
    "})\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf_param = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "}\n",
    "rf_clf =  GridSearchCV(rf, rf_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Random Forest\",\n",
    "    \"model\": rf_clf,\n",
    "    \"subsample\": 0.6,\n",
    "})\n",
    "\n",
    "# Extra Trees\n",
    "et = ExtraTreesClassifier()\n",
    "et_param = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "}\n",
    "et_clf =  GridSearchCV(et, et_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Extra Trees\",\n",
    "    \"model\": et_clf,\n",
    "    \"subsample\": 0.6,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d4782-3fa7-4a7d-a9cd-42af5542f950",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cv_models(models, x_test, y_test)\n",
    "test_cv_models(models, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24981f19-0db7-4a1e-8ae5-b503ce13eb75",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Including additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d30b32-5e24-4bbb-9413-b5d7879dff2c",
   "metadata": {},
   "source": [
    "We run the *en_core_web_sm* SpaCy pipeline that was used in the [Data Exploration section](#Data-Exploration) on the dataset.\n",
    "\n",
    "We also define a custom function to extract useful feature from the tokenized text and we apply it on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6526a1db-acd4-4b6a-8adf-671e59ad1ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tree):\n",
    "    features = []\n",
    "    for token in tree:\n",
    "        lemma = token.lemma_\n",
    "        pos_tag = token.pos_\n",
    "        dep_lab = token.dep_\n",
    "        head = token.head\n",
    "        if token.i < head.i:\n",
    "            direction = \"l\"\n",
    "        else:\n",
    "            direction = \"r\"\n",
    "        dfr = len(list(token.ancestors))\n",
    "        # if not token.is_stop:\n",
    "        features.append({\n",
    "            \"lem\": lemma ,\n",
    "            \"pos\": pos_tag, \n",
    "            \"dep\": dep_lab, \n",
    "            \"head\": head, \n",
    "            \"dir\": direction, \n",
    "            \"dfr\": dfr\n",
    "        })\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d073d-0124-4bd3-8aa0-3e345070689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SpaCy NLP pipeline on dataset\n",
    "parsed_df = df.copy()\n",
    "parsed_df[\"text\"] = df[\"text\"].apply(lambda x: nlp_model(x))\n",
    "\n",
    "# Extract useful features\n",
    "parsed_df[\"features\"] = parsed_df[\"text\"].apply(lambda x: extract_features(x))\n",
    "\n",
    "parsed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd28688-563f-44ad-bb26-00406bc1a501",
   "metadata": {},
   "source": [
    "Here is an example of a dependency parse tree extracted from a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75826fb4-bd12-4714-b095-cb806daafbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(parsed_df[\"text\"][0], jupyter=True, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ecefd-d1ee-48d3-8b9a-660e2e9f3b13",
   "metadata": {},
   "source": [
    "First of all, we try to exploit the **lemmatization** contained inside the SpaCy pipeline to check if it gets any better result than our rudimentary Bag of Words approach without any form of stemming.\n",
    "\n",
    "The same considerations mentioned previously about the vectorization of text apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecba5e9-32cf-4e21-a26e-a6ae5d0d5507",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.6, ngram_range=(2,2))\n",
    "parsed_df[\"text_lem\"] = parsed_df[\"features\"].apply(lambda x: \" \".join([t[\"lem\"] for t in x]))\n",
    "\n",
    "x, y = parsed_df[\"text_lem\"], parsed_df[\"label\"]\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = split(\n",
    "    x, y, test_size=0.2, val_size=0.0, seed=seed\n",
    ")\n",
    "\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887cea2c-d428-4d59-a1cb-4ad3b01aedc3",
   "metadata": {},
   "source": [
    "This time we only try Naive Bayes, Logistic Regression and a SVM; following the previous cross-validation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ce1ba9-7a27-4f04-a3fe-959698389e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb_param = {\"alpha\":[0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "nb_clf = GridSearchCV(nb, nb_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Naive Bayes\",\n",
    "    \"model\": nb_clf,\n",
    "})\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=100000)\n",
    "lr_param = [{\n",
    "    \"solver\": [\"liblinear\"], \n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"C\":[0.01, 0.1, 1, 10]\n",
    "},{\n",
    "    \"solver\": (\"lbfgs\", \"sag\", \"saga\"), \n",
    "    \"penalty\": [\"l2\"],\n",
    "    \"C\":[0.01, 0.1, 1]\n",
    "}]\n",
    "lr_clf = GridSearchCV(lr, lr_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Linear Regression\",\n",
    "    \"model\": lr_clf,\n",
    "    \"subsample\": 0.8,\n",
    "})\n",
    "\n",
    "# SVC\n",
    "svc = SVC()\n",
    "svc_param = {\"kernel\": [\"rbf\"], \"C\": [0.1, 1, 10]}\n",
    "svc_clf = GridSearchCV(svc, svc_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"SVC\", \n",
    "    \"model\": svc_clf, \n",
    "    \"subsample\": 0.7,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc5d1e-f893-458f-ba96-67732d96746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cv_models(models, x_test, y_test)\n",
    "test_cv_models(models, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44aecf7-0d90-4afe-9cd8-0324133198fd",
   "metadata": {},
   "source": [
    "By comparing the accuracies, there seems to be a slight improvement in the *SVM* case, but the results are quite symmetrical overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f459b-6ecd-497f-87ac-0141b68ecf3b",
   "metadata": {},
   "source": [
    "Now we can try to integrate the POS information and the average distance from root of each sentence for each word (ADFR) into our models, to do so we vectorize the tags using a **TfidfVectorizer** (noticing that a **Countvectorizer** would produce slightly better results for tree-based methods) for the POS tags and we normalize the ADFR using a **MinMaxScaler**.\n",
    "\n",
    "The various metrics are then concatenated into a single vector  and we apply the same models using our established cross-validation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f8f0d-1383-4409-8a65-a1c3996cec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize lemmas\n",
    "vectorizer = TfidfVectorizer(min_df=4, ngram_range=(2,2))\n",
    "x_train_lem = vectorizer.fit_transform(x_train_lem)\n",
    "x_test_lem = vectorizer.transform(x_test_lem)\n",
    "\n",
    "# Vectorize POS tags\n",
    "vectorizer = TfidfVectorizer(min_df=4)\n",
    "x_train_pos = vectorizer.fit_transform(x_train_pos)\n",
    "x_test_pos = vectorizer.transform(x_test_pos)\n",
    "\n",
    "# Normalize average distance from root\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "x_train_avgdfr = mms.fit_transform(x_train_avgdfr.values.reshape(-1, 1))\n",
    "x_test_avgdfr = mms.transform(x_test_avgdfr.values.reshape(-1, 1))\n",
    "\n",
    "x_train_avgdfr = sps.csr_matrix(x_train_avgdfr)\n",
    "x_test_avgdfr = sps.csr_matrix(x_test_avgdfr)\n",
    "\n",
    "# Concatenate vectors\n",
    "x_train = sps.hstack([x_train_lem, x_train_pos, x_train_avgdfr])\n",
    "x_test = sps.hstack([x_test_lem, x_test_pos, x_test_avgdfr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d2f0e-529a-4d6d-92dc-dac78c985cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb_param = {\"alpha\":[0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "nb_clf = GridSearchCV(nb, nb_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Naive Bayes\", \n",
    "    \"model\": nb_clf,\n",
    "})\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=100000)\n",
    "lr_param = [{\n",
    "    \"solver\": [\"liblinear\"], \n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"C\":[0.01, 0.1, 1, 10]\n",
    "},{\n",
    "    \"solver\": (\"lbfgs\", \"sag\", \"saga\"), \n",
    "    \"penalty\": [\"l2\"],\n",
    "    \"C\":[0.01, 0.1, 1]\n",
    "}]\n",
    "lr_clf = GridSearchCV(lr, lr_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Linear Regression\",\n",
    "    \"model\": lr_clf,\n",
    "    \"subsample\": 0.8,\n",
    "})\n",
    "\n",
    "# SVC\n",
    "svc = SVC()\n",
    "svc_param = {\"kernel\": [\"rbf\"], \"C\": [0.1, 1, 10]}\n",
    "svc_clf = GridSearchCV(svc, svc_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"SVC\", \n",
    "    \"model\": svc_clf,\n",
    "    \"subsample\": 0.7,\n",
    "})\n",
    "\n",
    "# Decision Tree\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree_param = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"], \n",
    "    \"max_features\": [None, \"sqrt\", \"log2\"],\n",
    "}\n",
    "dtree_clf =  GridSearchCV(dtree, dtree_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Decision Tree\",\n",
    "    \"model\": dtree_clf,\n",
    "})\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf_param = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"], \n",
    "}\n",
    "rf_clf =  GridSearchCV(rf, rf_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Random Forest\",\n",
    "    \"model\": rf_clf,\n",
    "    \"subsample\": 0.6,\n",
    "    \"usecached\": usecached,\n",
    "})\n",
    "\n",
    "# Extra Trees\n",
    "et = ExtraTreesClassifier()\n",
    "et_param = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"], \n",
    "}\n",
    "et_clf =  GridSearchCV(et, et_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Extra Trees\",\n",
    "    \"model\": et_clf,\n",
    "    \"subsample\": 0.6,\n",
    "    \"usecached\": usecached,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de2b7b7-2e01-468e-9fb5-01d019f40852",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cv_models(models, x_test, y_test)\n",
    "test_cv_models(models, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ec4ad-3ef0-4bfd-a24b-d470459cfb83",
   "metadata": {},
   "source": [
    "As we can see, the addition of these new features significantly improved the performance of more complex models, while models on the simpler side retained their accuracy found in previous experiments or returned worse results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702060c-8896-4682-bbf1-f28c2897bf55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Other approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7ac62-3b4b-4622-b9c6-939b5a6ce2f5",
   "metadata": {},
   "source": [
    "##### Neural network classifier\n",
    "\n",
    "Another approach that was experimented upon consisted in building a neural network classifier from scratch utilizing *pytorch* to perform classification on the Bag of Words representation (both augmented and non-augmented).\n",
    "\n",
    "The results were quite underwhelming with regards to the training time and resources spent, since in every scenario it performed similarly to the SVM.\n",
    "\n",
    "This approach will not be presented here but the source code used for the implementation of the clasifier can be found under the [corresponding section](#Preliminary-initialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fd4ef3-5aa7-4c70-93d0-3ed1d7e5d4ea",
   "metadata": {},
   "source": [
    "##### SVM on extracted features\n",
    "\n",
    "Training an SVM **only** on the features obtained in the [data exploration section](#Data-Exploration), it is possible to obtain an accuracy around 80%.\n",
    "\n",
    "This result is particularly interesting since no direct text information is given to the classifier, only derived statistics and metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c91aa2c-d936-4188-b61e-fa12789d4918",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Text embedding approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb977bc-92e5-4e7e-9743-36bc5abc6b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ed5d4f-7f7e-4302-a80e-89cb06d2cead",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Transformer-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7dd276-4653-4a2f-9040-3dd856ab08ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a167d75-a0f6-429f-8308-e859a068b9fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 2\n",
    "\n",
    "The challange is subdivided in two main tasks, the second task is a **Multinomial Classification** task that aims at identifying the specific *language model* that generated a given text passage, choosing from 6 different models labeled as A, B, C, D, E and F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1675e6a-e0b5-4f99-bcd7-88469e08d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification labels for Task 2\n",
    "labels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd205d9-bc58-4d98-97f6-a09601472f73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65458c-944d-463b-a7d9-6daef11b154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989117e8-6a2d-4eb2-99b3-73e44cd41277",
   "metadata": {},
   "source": [
    "### Bag of Words-based models and other approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5498498f-843f-40f3-acf6-c94204ebd027",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Only Bag of Words text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a707c9-ffc3-48e1-b94d-a9b84944c3d0",
   "metadata": {},
   "source": [
    "We load the english dataset for the first task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fd8019-9a82-4888-8ea3-b2307cf6fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import main dataset\n",
    "df = pd.read_csv(\"../AUTEXTIFICATION/subtask_2/train.tsv\", sep=\"\\t\")\n",
    "df = df.drop(\"id\", axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df1dc4-7758-4f16-869f-176eb93060ce",
   "metadata": {},
   "source": [
    "We use the same preprocessing function that we used for Task 1, we use it to preprocess our data, splitting it into train and test sets.\n",
    "\n",
    "All of the previous considerations still apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859943d1-8407-4878-88b2-816c3a65e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, lower=True, vectorizer=None, fit=True):\n",
    "    # Convert all text to lowercase\n",
    "    if lower:\n",
    "        data = [x.lower() for x in data]\n",
    "\n",
    "    # Remove punctuation and reset multiple spaces to one\n",
    "    punct_regex = re.compile(\"[\" + string.punctuation + \"\\’'\" + \"]\")\n",
    "    whitespace_regex = re.compile(\" ( )+\")\n",
    "    data = [whitespace_regex.sub(\" \", punct_regex.sub(\" \", x)) for x in data]\n",
    "    \n",
    "    # Vectorize\n",
    "    if vectorizer:\n",
    "        if fit:\n",
    "            data = vectorizer.fit_transform(data)\n",
    "        else:\n",
    "            data = vectorizer.transform(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334fbaeb-a923-4ebd-a38d-9959be41bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.6, ngram_range=(2,2))\n",
    "\n",
    "x, y = df[\"text\"], df[\"label\"]\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = split(\n",
    "    x, y, test_size=0.2, val_size=0.0, seed=seed\n",
    ")\n",
    "\n",
    "x_train = preprocess(x_train, vectorizer=vectorizer)\n",
    "x_test = preprocess(x_test, vectorizer=vectorizer, fit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3083c0d-d329-437e-ac42-67a8d1cea539",
   "metadata": {},
   "source": [
    "We define some simple models to try out the simple Bag of Words approach, without any additional data; the models that we are going to use are:\n",
    "- Multinomial Naive Bayes\n",
    "- Logistic Regression\n",
    "- C-Support SVM\n",
    "- Extra Tree classifier\n",
    "\n",
    "All of the previous models are run using 5-fold cross-validation on a small gridsearch around some of their default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dcaf8a-7e83-4b2f-9d6e-75f0ed0f4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb_param = {\"alpha\":[0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "nb_clf = GridSearchCV(nb, nb_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Naive Bayes\",\n",
    "    \"model\": nb_clf,\n",
    "})\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr_param = [{\n",
    "    \"solver\": [\"liblinear\"], \n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"C\":[0.01, 0.1, 1, 10]\n",
    "},{\n",
    "    \"solver\": (\"lbfgs\", \"sag\", \"saga\"), \n",
    "    \"penalty\": [\"l2\"],\n",
    "    \"C\":[0.01, 0.1, 1]\n",
    "}]\n",
    "lr_clf = GridSearchCV(lr, lr_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Linear Regression\",\n",
    "    \"model\": lr_clf,\n",
    "    \"subsample\": 0.9,\n",
    "})\n",
    "\n",
    "# SVC\n",
    "svc = SVC()\n",
    "svc_param = {\"kernel\": [\"rbf\"], \"C\": [0.1, 1, 10]}\n",
    "svc_clf = GridSearchCV(svc, svc_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"SVC\",\n",
    "    \"model\": svc_clf,\n",
    "    \"subsample\": 0.6,\n",
    "})\n",
    "\n",
    "# Extra Trees\n",
    "et = ExtraTreesClassifier()\n",
    "et_param = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "}\n",
    "et_clf =  GridSearchCV(et, et_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Extra Trees\",\n",
    "    \"model\": et_clf,\n",
    "    \"subsample\": 0.6,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddd3156-1361-46f0-8189-c15c4054a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cv_models(models, x_test, y_test)\n",
    "test_cv_models(models, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bf6a32-2f35-4ca3-a069-f54a2b424e7e",
   "metadata": {},
   "source": [
    "From the results we see a much worse performance with respect to Task 1, this is expected, as Task 2 is inherently more difficult due to the presence of more classes and the \"more similar\" nature of those classes.\n",
    "\n",
    "Our simple Bag of Words classifiers still have an edge against a random classifier since all of them can abundantly surpass the 1/6 ≈ 17% baseline.\n",
    "\n",
    "From the confusion matrix results we confirm the similarities between A, B, C and D, E, F respectively, in addition we can observe that F is much more easier to detect than the other classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576bc44-0072-4f5e-b186-42e661fc3c05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Including additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f523224b-6bd6-47d6-b877-fd00b43853a1",
   "metadata": {},
   "source": [
    "We run the *en_core_web_sm* SpaCy pipeline and define the custom feature extraction function that were used in the previous points.\n",
    "\n",
    "This time we skip directly to the application of augmentation features, without dwelling on the benefits of lemmatization.\n",
    "\n",
    "The techniques applied through this section are the same as those applied for Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db117f8-084e-4a9f-88ca-e091c6af767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tree):\n",
    "    features = []\n",
    "    for token in tree:\n",
    "        lemma = token.lemma_\n",
    "        pos_tag = token.pos_\n",
    "        dep_lab = token.dep_\n",
    "        head = token.head\n",
    "        if token.i < head.i:\n",
    "            direction = \"l\"\n",
    "        else:\n",
    "            direction = \"r\"\n",
    "        dfr = len(list(token.ancestors))\n",
    "        # if not token.is_stop:\n",
    "        features.append({\n",
    "            \"lem\": lemma ,\n",
    "            \"pos\": pos_tag, \n",
    "            \"dep\": dep_lab, \n",
    "            \"head\": head, \n",
    "            \"dir\": direction, \n",
    "            \"dfr\": dfr\n",
    "        })\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef96b8-3c4c-45ba-a131-3fa8a785edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SpaCy NLP pipeline on dataset\n",
    "parsed_df = df.copy()\n",
    "parsed_df[\"text\"] = df[\"text\"].apply(lambda x: nlp_model(x))\n",
    "\n",
    "# Extract useful features\n",
    "parsed_df[\"features\"] = parsed_df[\"text\"].apply(lambda x: extract_features(x))\n",
    "\n",
    "parsed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42977250-0709-4ff2-a53e-c5ee9d35cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lem = parsed_df[\"features\"].apply(lambda x: \" \".join([t[\"lem\"] for t in x]))\n",
    "x_pos = parsed_df[\"features\"].apply(lambda x: \" \".join([t[\"pos\"] for t in x]))\n",
    "x_avgdfr = parsed_df[\"features\"].apply(lambda x: sum(t[\"dfr\"] for t in x) / len(x))\n",
    "y = parsed_df[\"label\"]\n",
    "\n",
    "x_train_lem, x_val_lem, x_test_lem, y_train, y_val, y_test = split(\n",
    "    x_lem, y, test_size=0.2, val_size=0.0, seed=seed\n",
    ")\n",
    "x_train_pos, x_val_pos, x_test_pos, y_train, y_val, y_test = split(\n",
    "    x_pos, y, test_size=0.2, val_size=0.0, seed=seed\n",
    ")\n",
    "x_train_avgdfr, x_val_avgdfr, x_test_avgdfr, y_train, y_val, y_test = split(\n",
    "    x_avgdfr, y, test_size=0.2, val_size=0.0, seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53703422-52ae-4ece-92b0-bb435fa83417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize lemmas\n",
    "vectorizer = TfidfVectorizer(min_df=4, ngram_range=(2,2))\n",
    "x_train_lem = vectorizer.fit_transform(x_train_lem)\n",
    "x_test_lem = vectorizer.transform(x_test_lem)\n",
    "\n",
    "# Vectorize POS tags\n",
    "vectorizer = TfidfVectorizer(min_df=4)\n",
    "x_train_pos = vectorizer.fit_transform(x_train_pos)\n",
    "x_test_pos = vectorizer.transform(x_test_pos)\n",
    "\n",
    "# Normalize average distance from root\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "x_train_avgdfr = mms.fit_transform(x_train_avgdfr.values.reshape(-1, 1))\n",
    "x_test_avgdfr = mms.transform(x_test_avgdfr.values.reshape(-1, 1))\n",
    "\n",
    "x_train_avgdfr = sps.csr_matrix(x_train_avgdfr)\n",
    "x_test_avgdfr = sps.csr_matrix(x_test_avgdfr)\n",
    "\n",
    "# Concatenate vectors\n",
    "x_train = sps.hstack([x_train_lem, x_train_pos, x_train_avgdfr])\n",
    "x_test = sps.hstack([x_test_lem, x_test_pos, x_test_avgdfr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7ba8f5-e5bb-491d-add4-f4a21c2e39ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb_param = {\"alpha\":[0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "nb_clf = GridSearchCV(nb, nb_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Naive Bayes\", \n",
    "    \"model\": nb_clf,\n",
    "})\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=100000)\n",
    "lr_param = [{\n",
    "    \"solver\": [\"liblinear\"], \n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"C\":[0.01, 0.1, 1, 10]\n",
    "},{\n",
    "    \"solver\": (\"lbfgs\", \"sag\", \"saga\"), \n",
    "    \"penalty\": [\"l2\"],\n",
    "    \"C\":[0.01, 0.1, 1]\n",
    "}]\n",
    "lr_clf = GridSearchCV(lr, lr_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Linear Regression\",\n",
    "    \"model\": lr_clf,\n",
    "    \"subsample\": 0.8,\n",
    "})\n",
    "\n",
    "# SVC\n",
    "svc = SVC()\n",
    "svc_param = {\"kernel\": [\"rbf\"], \"C\": [0.1, 1, 10]}\n",
    "svc_clf = GridSearchCV(svc, svc_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"SVC\", \n",
    "    \"model\": svc_clf,\n",
    "    \"subsample\": 0.7,\n",
    "})\n",
    "\n",
    "# Decision Tree\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree_param = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"], \n",
    "    \"max_features\": [None, \"sqrt\", \"log2\"],\n",
    "}\n",
    "dtree_clf =  GridSearchCV(dtree, dtree_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Decision Tree\",\n",
    "    \"model\": dtree_clf,\n",
    "})\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf_param = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"], \n",
    "}\n",
    "rf_clf =  GridSearchCV(rf, rf_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Random Forest\",\n",
    "    \"model\": rf_clf,\n",
    "    \"subsample\": 0.6,\n",
    "})\n",
    "\n",
    "# Extra Trees\n",
    "et = ExtraTreesClassifier()\n",
    "et_param = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"], \n",
    "}\n",
    "et_clf =  GridSearchCV(et, et_param, cv=5, scoring=\"f1_micro\", verbose=1)\n",
    "models.append({\n",
    "    \"name\": \"Extra Trees\",\n",
    "    \"model\": et_clf,\n",
    "    \"subsample\": 0.6,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581fd474-aaac-4ab5-b739-3d54a1eab414",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cv_models(models, x_test, y_test)\n",
    "test_cv_models(models, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3718009-3c96-4364-b41a-ff9ed00a2dc3",
   "metadata": {},
   "source": [
    "We can observe that for Task 2, the improvements of adding additional information are much less evident (although still present), even for complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dcbed7-c47f-494a-9410-5a479962a312",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Text embedding approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c58057-ce5a-472d-b906-be76f5deae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60977ce7-7d35-43f7-8cfb-f282af480df4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Transformer-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8345c040-bac1-4a35-a276-30adfc430e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57059c84-45b4-432c-affc-9c18ecddadb5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ff298-77a6-4ebf-8b3e-75b0df5ad52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
